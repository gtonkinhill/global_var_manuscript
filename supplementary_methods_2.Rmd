---
title: "Supplementary Methods (Mosaic)"
author: "Gerry Tonkin-Hill"
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 12
    fig_height: 8
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8,
                      echo=FALSE, warning=FALSE, message=FALSE,
                      tidy=TRUE)
```

##Mosaic model
The mosaic model is best run on protein sequence as the alignments will be more interpretable. It will also be more efficient as the algorithm complexity is of order $N^2 l^2$. Furthermore, we only want to compare isolates with isolates from other regions. Thus we remove lab isolates and those sequences that can not be translated. We say a sequence can be translated if the resulting protein sequence has no stop codons.

First we translate all sequences to identify those that can not be succesfully translated.
```{python, eval=FALSE}
from mungo.sequence import sixFrameTranslation
from mungo.fasta import FastaReader
import os, sys
from collections import defaultdict

num_stops = 0
seqCount = 0
badSeqs = 0
bad_lengths = []

inputfile = "./mosaic_data/combined_454_tessema.fas"
output_file = "./mosaic_data/Protein_combined_454_tessema_renamed.fasta"

with open(output_file + "_BadSeqs", 'w') as badfile:
  with open(output_file, 'w') as outfile:
    for h,s in FastaReader(inputfile):
      stops = 9999
      translation = sixFrameTranslation(s)
      for frame in translation:
        st = translation[frame].count('*')
        if st < stops:
          best = frame
          stops = st
      if stops <= num_stops:
        outfile.write(">" + h + " frame_" + str(best) + "\n")
        outfile.write(translation[best] + "\n")
      else:
        badSeqs += 1
        bad_lengths.append(len(s))
        badfile.write(">" + h + "\n")
        badfile.write(s + "\n")
      seqCount += 1

print (str((100.0*badSeqs)/seqCount) + "percent or "
  + str(badSeqs) + " out of " + str(seqCount) + " were not translated.")
```

97.14% of the sequences were successfully translated. 
We would now like to filter out those that did not translate along with the labratory isolates before clustering the remaining sequences.

```{python, eval=FALSE}
from mungo.fasta import FastaReader

successfully_translated = {}
keep=set()

for h,s in FastaReader("./mosaic_data/Protein_combined_454_tessema_renamed.fasta"):
  successfully_translated[h]=s

#Remove lab isolates
with open("./mosaic_data/Protein_NoLab_combined_454_tessema.fasta", 'w') as outfile:
  for h in successfully_translated:
    if "DD2" in h.split(".")[0]: continue
    if "HB3" in h.split(".")[0]: continue
    if "3D7" in h.split(".")[0]: continue
    outfile.write(">"+h+"\n"+successfully_translated[h]+"\n")
    keep.add(h.split()[0])

#Create filtered DNA sequence file for clustering
with open("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/combined_454_tessema.fas"):
    if h in keep:
      outfile.write(">"+h+"\n"+s+"\n") 
```

We only want to look at a single copy of DBLa type as this will drastically reduce the computational complexity of the problem. We cluster the filtered DNA sequences at 96% id using the same pipline as was used in the binary type analysis.

```{bash, eval=FALSE}
python ./scripts/clusterDBLa.py -o ./mosaic_data/ -r ./mosaic_data/DNA_NoLab_translateable_combined_454_tessema.fasta --cpu 30 --verbose
```

We now extract the protein sequences that correspond to the centroids of the clusters.

```{python, eval=FALSE}
from mungo.fasta import FastaReader

centroids=set()
for h,s in FastaReader("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_centroids.fasta"):
  centroids.add(h.split(";sample=")[0])

with open("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/Protein_NoLab_combined_454_tessema.fasta"):
    if h.split()[0] in centroids:
      outfile.write(">"+h+"\n"+s+"\n")
```

The sequences from Tessema et al cover a larger region of the DBLa domain. In inferring the mixtures of each isolate we will later assume that each sequence is covering approximately the same region. Cosequently, we need to trim the sequences of Tessema et al to match those in the 454 data. This was not important in the DNA clustering as the pipeline we used allowed for terminal gaps at no cost. To achieve this we align all the protein sequences using gismo which was found to handle VAR protein sequences better than many other aligners.

```{bash, eval=FALSE}
./scripts/gismo_unlimited \ ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta \ -maxseq=40000
```

The trailing sequence from the PNG reads will be trimmed in this alignment and we will be left with a consensus region from. 

```{python, eval=FALSE}
from mungo.fasta import FastaReader

count = 0
with open("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta.fa"):
    count+=1
    if count<=1: continue
    h=h.split()[0]
    s=s.replace("-","")
    s=s.upper()
    outfile.write(">"+h+"\n"+s+"\n")
```

Now we have representative protein sequences for each DBLa type we can start to fit the jumpping hidden Markov model and the FFp method.

##Alignment free comparison with FFP

First we need to split the sequences into seperate fasta files for each isolate
```{bash, eval=FALSE}
mkdir ffp_data
cd ffp_data
cp ../processed_data/combined_454_tessema.fas ./
cd ..
```

```{python, eval=FALSE}
from mungo.fasta import FastaReader
from collections import defaultdict

isolates = defaultdict(list)

for h,s in FastaReader("./processed_data/combined_454_tessema.fas"):
  isolates[h.split(".")[0]].append((h,s))

for iso in isolates:
  with open("./processed_data/"+iso+".fasta",'w') as outfile:
    for s in isolates[iso]:
      outfile.write(">"+s[0]+"\n"+s[1]+"\n")
```

We would now like to decide on an appropriate choice for the l-mer length. To investigate this we use the centroids after clustering the 3D7 isolate reads as a reference.
```{bash, eval=FALSE}
cd ffp_data
python ../scripts/clusterDBLa.py -o ./ -r 3D7.fasta

#Construct a word usage (vocabulary) profile
ffpvprof -e 40 -f 2 3D7_renamed_centroids.fasta > ../processed_data/ffp_word_usage.txt

#Construct a relative entropy profile
ffpreprof -e 40 3D7_renamed_centroids.fasta > ../processed_data/ffp_entropy_profile.txt

#We no longer need the lab isolates and can remove them
rm *3D7*.fasta
rm *DD2*.fasta
rm *HB3*.fasta

cd ..
```

We can now attempt to choose an appropriate value of l. First let look at word usage to get an idea of a lower bound.
```{r}
word_usage <- fread("./processed_data/ffp_word_usage.txt",
                    data.table = FALSE)
plot(word_usage)
```

```{r}
entropy <- fread("./processed_data/ffp_entropy_profile.txt",
                    data.table = FALSE)
plot(entropy)
```

Thus a choice of l=20 appears to be appropriate.

We now want to get the set of sequences (excluding the labratory isolates) from which to cluster. We borrow those set up for the jumping HMM analysis (see supplementary_methods_2).

We can now run a script to calculate the ffp distance matrix. The original ffp script from xxx generated segment faults when we attempted to use it.
```{bash, eval=FALSE}
python ./scripts/ffp.py --kmer_length 20 --out ./processed_data/ffp_distance_matrix.phylip --seq ./processed_data/combined_454_tessema.fas --verbose
```

Finally a tree was built using fastme v2.1.4 with default parameters in interactive mode. We can now have a look at the resulting tree.

```{r}
#Read in data required for plotting
ffp <- read.tree("./processed_data/ffp_distance_matrix.phylip_fastme_tree.txt")
isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))
#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]


isolateInformation$Country[isolateInformation$Country %in% c("3D7", "3D7xDD2", "DD2", "DD2xHB3", "HB3", "HB3xDD2")] <- "LabIsolate"
groupInfo <- isolateInformation %>% group_by(Country) %>%
  do(taxa_list = .$Isolate)
# groupInfo <- groupInfo[!(groupInfo$Country %in% c("3D7", "3D7xDD2", "DD2", "DD2xHB3", "HB3", "HB3xDD2")),]

groups <- lapply(groupInfo$taxa_list, as.vector)
names(groups) <- groupInfo$Country
ffp <- groupOTU(ffp, groups)

gg <- ggtree(ffp, aes(color=group, label=node)
             , size=0.3, branch.length = "none", layout="circular")
gg <- gg + scale_color_manual(values=cols,
                              labels=names(groups)) +
  theme(legend.position="right")
gg
```


##Jumping hidden Markov Model

Unfortunately the complete Baum-Welch algorithm  requires a number of full all-vs-all searches using the forward-backward algorithms and this is very computaitonaly instesive. Due to computational contraints this was impractable. Consequently, the slightly less accurate but much fast viterbi training algorithm was used.

We first split the dataset into subset to be searched in parralel on our computing cluster.
```{bash, eval=FALSE}
mkdir mosaic_processed_data
cp ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta ./mosaic_processed_data/
cd mosaic_processed_data
python ../scripts/split_into_runs.py Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta
cd ..
```

Now we perform the Viterbi training algorithm
Iterate until convergence:
0) Choose an initial set of parameters
1) Compute the Viterbi paths of all sequences
2) Count frequencies of events and calculate parameters
3) Update -> 1) 
4) Stop when the major paramters del and eps change by less than 1%. 

The algorithm converged after 7 iterations.

This gives us the non-jump parameters which we fix when estimating a likelihood surface for the jump parameter. Due to computational constraints we could not estimate the jump parameter by computing and all-vs-all search at each parameter point. Thus we resort to searching 1000 randomly chosen target sequences against the entire dataset. That is 1000-vs-all. First, 1000 sequences are randomly selected as targets.

```{bash, eval=FALSE}
cd mosaic_processed_data
python ../scripts/random_target_sample.py Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta
cd ..
```

```{bash, eval=FALSE}
import glob
from collections import defaultdict

jump_llk = defaultdict(list)
for logfile in glob.glob("*jump*_output.log"):
  jump = float(logfile.split("jump")[1].split("_output")[0])
  with open(logfile, 'rU') as infile:
    for line in infile:
      if "Maximum log-likelihood =" in line:
          jump_llk[jump].append(float(line.split("=")[1].strip()))

with open("jump_llk.csv", 'w') as outfile:
  for j in sorted(jump_llk.keys()):
    outfile.write(",".join([str(j)]+[str(l) for l in jump_llk[j]]) + "\n")
```

Lets investigate the jump log-likelihoods
```{r}
library(data.table)
library(ggplot2)
library(boot)

llk <- read.table("./mosaic_data/jump_llk.csv"
                  , sep=",", header = FALSE, fill = TRUE)

llk_matrix <- data.matrix(llk[,2:ncol(llk)])

rownames(llk_matrix) <- llk[,1]

sum_stat <- function(x,i){
  t<-x[i]
  sum(t, na.rm = TRUE)
}

llk_df <- data.frame(jump=rownames(llk_matrix),
           llk=rowSums(llk_matrix, na.rm = TRUE),
           CI.L=unlist(apply(llk_matrix, 1
              , function(x){boot.ci(boot(x, sum_stat, 1000),type="perc")$percent[[4]]} )),
           CI.R=unlist(apply(llk_matrix, 1
              , function(x){boot.ci(boot(x, sum_stat, 1000),type="perc")$percent[[5]]} )),
           stringsAsFactors = FALSE)

ggplot(llk_df, aes(x=jump, y=llk, group=1)) +
    geom_line() +
    geom_errorbar(width=.1, aes(ymin=CI.L, ymax=CI.R)) +
    geom_point(shape=21, size=3, fill="white") + 
    ylim(c(-110000,-90000))

rownames(llk_matrix)[which.max(rowSums(llk_matrix, na.rm = TRUE))]
```