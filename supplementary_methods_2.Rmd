---
title: "Supplementary Methods (Mosaic)"
author: "Gerry Tonkin-Hill"
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 12
    fig_height: 8
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8,
                      echo=TRUE, warning=FALSE, message=FALSE,
                      tidy=TRUE)
```

##Load libraries
```{r}
library(data.table)
library(stringr)
library(readr)
library(ggplot2)
library(boot)
library(ggcorrplot)
library(reshape2)
library(dplyr)
```

##Mosaic model
The mosaic model is best run on protein sequence as the alignments will be more interpretable. It will also be more efficient as the algorithm complexity is of order $N^2 l^2$. Furthermore, we only want to compare isolates with isolates from other regions. Thus we remove lab isolates and those sequences that can not be translated. We say a sequence can be translated if the resulting protein sequence has no stop codons.

First we translate all sequences to identify those that can not be successfully translated.
```{python, eval=FALSE}
from mungo.sequence import sixFrameTranslation
from mungo.fasta import FastaReader
import os, sys
from collections import defaultdict

num_stops = 0
seqCount = 0
badSeqs = 0
bad_lengths = []

inputfile = "./mosaic_data/combined_454_tessema.fas"
output_file = "./mosaic_data/Protein_combined_454_tessema_renamed.fasta"

with open(output_file + "_BadSeqs", 'w') as badfile:
  with open(output_file, 'w') as outfile:
    for h,s in FastaReader(inputfile):
      stops = 9999
      translation = sixFrameTranslation(s)
      for frame in translation:
        st = translation[frame].count('*')
        if st < stops:
          best = frame
          stops = st
      if stops <= num_stops:
        outfile.write(">" + h + " frame_" + str(best) + "\n")
        outfile.write(translation[best] + "\n")
      else:
        badSeqs += 1
        bad_lengths.append(len(s))
        badfile.write(">" + h + "\n")
        badfile.write(s + "\n")
      seqCount += 1

print (str((100.0*badSeqs)/seqCount) + "percent or "
  + str(badSeqs) + " out of " + str(seqCount) + " were not translated.")
```

97.14% of the sequences were successfully translated. 
We would now like to filter out those that did not translate along with the laboratory isolates before clustering the remaining sequences.

```{python, eval=FALSE}
from mungo.fasta import FastaReader

successfully_translated = {}
keep=set()

for h,s in FastaReader("./mosaic_data/Protein_combined_454_tessema_renamed.fasta"):
  successfully_translated[h]=s

#Remove lab isolates
with open("./mosaic_data/Protein_NoLab_combined_454_tessema.fasta", 'w') as outfile:
  for h in successfully_translated:
    if "DD2" in h.split(".")[0]: continue
    if "HB3" in h.split(".")[0]: continue
    if "3D7" in h.split(".")[0]: continue
    outfile.write(">"+h+"\n"+successfully_translated[h]+"\n")
    keep.add(h.split()[0])

#Create filtered DNA sequence file for clustering
with open("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/combined_454_tessema.fas"):
    if h in keep:
      outfile.write(">"+h+"\n"+s+"\n") 
```

We only want to look at a single copy of DBLa type as this will drastically reduce the computational complexity of the problem. We cluster the filtered DNA sequences at 96% id using the same pipeline as was used in the binary type analysis.

```{bash, eval=FALSE}
python ./scripts/clusterDBLa.py -o ./mosaic_data/ -r ./mosaic_data/DNA_NoLab_translateable_combined_454_tessema.fasta --cpu 30 --verbose
```

We now extract the protein sequences that correspond to the centroids of the clusters.

```{python, eval=FALSE}
from mungo.fasta import FastaReader

centroids=set()
for h,s in FastaReader("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_centroids.fasta"):
  centroids.add(h.split(";sample=")[0])

with open("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/Protein_NoLab_combined_454_tessema.fasta"):
    if h.split()[0] in centroids:
      outfile.write(">"+h+"\n"+s+"\n")
```

The sequences from Tessema et al cover a larger region of the DBLa domain. In inferring the mixtures of each isolate we will later assume that each sequence is covering approximately the same region. Consequently, we need to trim the sequences of Tessema et al to match those in the 454 data. This was not important in the DNA clustering as the pipeline we used allowed for terminal gaps at no cost. To achieve this we align all the protein sequences using gismo which was found to handle VAR protein sequences better than many other aligners.

```{bash, eval=FALSE}
./scripts/gismo_unlimited \ ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta \ -maxseq=40000
```

The trailing sequence from the PNG reads will be trimmed in this alignment and we will be left with a consensus region from. 

```{python, eval=FALSE}
from mungo.fasta import FastaReader

count = 0
with open("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta.fa"):
    count+=1
    if count<=1: continue
    h=h.split()[0]
    s=s.replace("-","")
    s=s.upper()
    outfile.write(">"+h+"\n"+s+"\n")
```

Now we have representative protein sequences for each DBLa type we can start to fit the jumping hidden Markov model. Unfortunately the complete Baum-Welch algorithm  requires a number of full all-vs-all searches using the forward-backward algorithms and this is very computationaly intensive. Due to computational constraints this was impracticable. Consequently, the slightly less accurate but much faster viterbi training algorithm was used.

We first split the data set into subset to be searched in parallel on our computing cluster.
```{bash, eval=FALSE}
mkdir mosaic_processed_data
cp ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta ./mosaic_processed_data/
cd mosaic_processed_data
python ../scripts/split_into_runs.py Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta
cd ..
```

Now we perform the Viterbi training algorithm
Iterate until convergence:
0) Choose an initial set of parameters
1) Compute the Viterbi paths of all sequences
2) Count frequencies of events and calculate new parameters
3) Update -> 1) 
4) Stop when the major parameters del and eps change by less than 1%. 

The algorithm converged after 7 iterations.

Each iteration was run as on a high performing computing cluster. The scripts used for this stage are available in the hpc_scripts sub folder.
```{bash, eval=FALSE}
for FILE in *_run*.fasta;
do
fasta=`basename $1`
mosaic -seq $1 -aa -tag $fasta -group 2 db target -target target -del del -eps eps -rec 0.0 > ${fasta}_output.log
done
```

The `estimate_transition_probs_frm_viterbi.py` script was then used to estimate the updated paramters between runs and the mosaic code was recompiled with the updated paramters.

This gives us the non-jump parameters which we fix when estimating a likelihood surface for the jump parameter. Due to computational constraints we could not estimate the jump parameter by computing an all-vs-all search at each parameter point. Thus we resort to searching 1000 randomly chosen target sequences against the entire data set. That is 1000-vs-all. First, 1000 sequences are randomly selected as targets.

```{bash, eval=FALSE}
cd mosaic_processed_data
python ../scripts/random_target_sample.py Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta
cd ..
```

Now run mosaic for different jump parameters to estimate a likelihood surface. This was also run on a HPC but an outline of the commands is given below.

```{bash, eval=FALSE}
for jump in $(seq 0.00 0.001 0.1);
do

fasta=Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion_randomSampleTargetSize1000.fasta

tag=Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion_randomSampleTargetSize1000_jump${jump}

mosaic -seq $fasta -aa -tag $tag -group 2 db target -target target -del 0.0166765773591 -eps 0.273305573191 -rec $jump > /home/tonkin-hill.g/full_mosaic_run/results_jump/${tag}_output.log

done
```

```{bash, eval=FALSE}
import glob
from collections import defaultdict

jump_llk = defaultdict(list)
for logfile in glob.glob("*jump*_output.log"):
  jump = float(logfile.split("jump")[1].split("_output")[0])
  with open(logfile, 'rU') as infile:
    for line in infile:
      if "Maximum log-likelihood =" in line:
          jump_llk[jump].append(float(line.split("=")[1].strip()))

with open("jump_llk.csv", 'w') as outfile:
  for j in sorted(jump_llk.keys()):
    outfile.write(",".join([str(j)]+[str(l) for l in jump_llk[j]]) + "\n")
```

Lets investigate the jump log-likelihoods
```{r}
llk <- read.table("./mosaic_data/jump_llk.csv"
                  , sep=",", header = FALSE, fill = TRUE)

llk_matrix <- data.matrix(llk[,2:ncol(llk)])

rownames(llk_matrix) <- llk[,1]

sum_stat <- function(x,i){
  t<-x[i]
  sum(t, na.rm = TRUE)
}

llk_df <- data.frame(jump=rownames(llk_matrix),
           llk=rowSums(llk_matrix, na.rm = TRUE),
           CI.L=unlist(apply(llk_matrix, 1
              , function(x){boot.ci(boot(x, sum_stat, 1000),type="perc")$percent[[4]]} )),
           CI.R=unlist(apply(llk_matrix, 1
              , function(x){boot.ci(boot(x, sum_stat, 1000),type="perc")$percent[[5]]} )),
           stringsAsFactors = FALSE)

ggplot(llk_df, aes(x=jump, y=llk, group=1)) +
    geom_line() +
    geom_errorbar(width=.1, aes(ymin=CI.L, ymax=CI.R)) +
    geom_point(shape=21, size=3, fill="white") + 
    ylim(c(-110000,-90000))

rownames(llk_matrix)[which.max(rowSums(llk_matrix, na.rm = TRUE))]
```

We can now run the full model in an all vs all fashion. This was again run on a HPC cluster.
```{bash, eval=FALSE}
for FILE in *_run*.fasta;
do
fasta=`basename $1`
mosaic -ma -seq $1 -aa -tag $fasta -group 2 db target -target target -del 0.0166765773591 -eps 0.273305573191 -rec 0.014 > ${fasta}_output.log
done
```

Now calculate the proportions

```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/proportions
mkdir ./mosaic_processed_data/proportions/temp_files
python ./scripts/calculate_expected_proportions.py --otu ./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary.txt --map ./mosaic_processed_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta_mapping.txt --out_dir ./mosaic_processed_data/proportions/ --pos ./mosaic_processed_data/results_full/*post.txt --verbose --compute_all --temp_dir ./mosaic_processed_data/proportions/temp_files/ --seq ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta
```

Load proportions

```{r}
isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))

#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]
isolateInformation <- isolateInformation[,c("Isolate", "Country")]

#Proportion file
prop_files <- Sys.glob("./mosaic_processed_data/proportions/*proportions.txt")
avg_proportions <- data.frame()
for (f in prop_files){

  proportion <- fread(f, data.table = FALSE)
  colnames(proportion) <- c("Target", "Reference", "Proportion")
prop_merge <- merge(proportion, isolateInformation, by.x="Reference", by.y = "Isolate", all.x=TRUE)

  sum_location <- prop_merge %>% group_by(Country) %>%
    summarise(
      num.isolates = n(),
      sum.prop = sum(Proportion),
      avg.proportion = mean(Proportion)
    )

  sum_location$reWeighted <- sum_location$avg.proportion/sum(
    sum_location$avg.proportion)
  sum_location$Country <- factor(sum_location$Country
                                       , levels = c("Ape", "Uganda", "Ghana",
                                                    "Gabon", "Iran", "Thailand",
                                                    "PNG", "Peru", "FrenchGuiana",
                                                    "Venezuela", "Colombia"))
  iso <- gsub(".*proportions/","",f)
  iso <- gsub("_proportions.txt","",iso)
  sum_location$Isolate <- rep(iso, nrow(sum_location))
  avg_proportions <- rbind(avg_proportions, sum_location)
}

colnames(isolateInformation) <- c("Isolate","Origin")
avg_proportions <- merge(avg_proportions, isolateInformation, by.x="Isolate", by.y="Isolate", all.x=TRUE)

avg_proportions$Origin <- factor(avg_proportions$Origin, levels = levels(avg_proportions$Country))

ggplot(avg_proportions, aes(x=Isolate, y=reWeighted, fill=Country)) + 
  facet_grid(Country~Origin, scales = "free_x", space = "free_x") + 
  coord_cartesian(ylim=c(0,1)) + 
  geom_bar(stat = "identity") + 
  theme_bw() + theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  ylab("Inferred Mixture Proportion") + 
  theme(strip.text.x = element_text(angle=90),
        strip.text.y = element_text(angle=0)) +
  theme(text = element_text(size=15))

```

```{r}
colombia_props <- avg_proportions[avg_proportions$Origin=="Colombia",]
colombia_africa_props <- colombia_props[colombia_props$Country %in% c("Uganda", "Gabon", "Ghana"),]
colombia_africa_props <- colombia_africa_props %>%
  group_by(Isolate) %>%
  summarise(africa_sum_reWeighted = sum(reWeighted))
colombia_frenchGuiana_Africa <- colombia_props[colombia_props$Country == "FrenchGuiana",]
colombia_frenchGuiana_Africa <- colombia_frenchGuiana_Africa[,c("Isolate","reWeighted")]
colombia_frenchGuiana_Africa <- merge(colombia_frenchGuiana_Africa, colombia_africa_props,
                                      by.x="Isolate", by.y = "Isolate",
                                      all = TRUE)

wilcox.test(colombia_frenchGuiana_Africa$reWeighted,
            colombia_frenchGuiana_Africa$africa_sum_reWeighted,
            paired = TRUE)
```

```{r}
summary_prop_table <- avg_proportions %>% group_by_(.dots=c("Origin","Country")) %>%
  summarise(
    mean=mean(reWeighted),
    sd=sd(reWeighted)
  )
levels = c("Uganda", "Ghana","Gabon", "Iran", "Thailand","PNG", "Peru"
           , "FrenchGuiana","Venezuela", "Colombia")
summary_prop_table$Origin <- factor(summary_prop_table$Origin
                                    , levels=levels)
summary_prop_table$Country <- factor(summary_prop_table$Country
                                    , levels=levels)

gg <- ggplot(summary_prop_table, aes(Country, Origin, fill = mean)) 
gg <- gg + geom_raster()
gg <- gg + scale_fill_gradientn(colours=c("#FFFFFFFF","#FF0000FF"))
gg <- gg + geom_text(data=summary_prop_table,aes(x=Country, y=Origin
                    ,label=paste(sprintf("%0.3f", round(mean, digits = 3))
                                 , sprintf("%0.3f", round(sd, digits = 3))
                                 , sep = "±"))
                    , size=3)
gg <- gg + theme(axis.text.x = element_text(angle = 45, hjust = 1))
gg <- gg + theme(text = element_text(size=15))
gg
```


Now plot without matching to themselves
```{r}
avg_proportions_noSelf <- avg_proportions[avg_proportions$Country!=avg_proportions$Origin,]

gg <- ggplot(avg_proportions_noSelf, aes(x=Isolate, y=reWeighted, fill=Country))
gg <- gg + facet_grid(Country~Origin, scales = "free_x", space = "free_x")
gg <- gg + coord_cartesian(ylim=c(0,0.5))
gg <- gg + geom_bar(stat = "identity") + 
  theme_bw() + theme(axis.text.x=element_blank(),
                     axis.ticks.x=element_blank()) +
  ylab("Inferred Mixture Proportion") + 
  theme(strip.text.x = element_text(angle=90),
        strip.text.y = element_text(angle=0)) +
  theme(text = element_text(size=15))
gg
```

##Ape parasite

It is also interesting to look out how the Ape reichenowi isolate from Larremore et al looks.
First we need to reformat the reichenowi sequences.

```{python, eval=FALSE}
from mungo.fasta import FastaReader
from mungo.sequence import sixFrameTranslation
import os, sys
from collections import defaultdict

with open("./processed_data/recheniwoi_from_larremore_renamed.fasta", 'w') as outfile:
  for h,s in FastaReader("./data/all_recheniwoi_from_larremore.fasta"):
    h=h.replace(",","")
    h="_".join(h.split())
    outfile.write(">SYpttt15." + h + "\n" + s + "\n")

num_stops = 0
seqCount = 0
badSeqs = 0
bad_lengths = []

inputfile = "./processed_data/recheniwoi_from_larremore_renamed.fasta"
output_file = "./mosaic_data/Protein_recheniwoi_from_larremore_renamed.fasta"

with open(output_file + "_BadSeqs", 'w') as badfile:
  with open(output_file, 'w') as outfile:
    for h,s in FastaReader(inputfile):
      stops = 9999
      translation = sixFrameTranslation(s)
      for frame in translation:
        st = translation[frame].count('*')
        if st < stops:
          best = frame
          stops = st
      if stops <= num_stops:
        outfile.write(">" + h + " frame_" + str(best) + "\n")
        outfile.write(translation[best] + "\n")
      else:
        badSeqs += 1
        bad_lengths.append(len(s))
        badfile.write(">" + h + "\n")
        badfile.write(s + "\n")
      seqCount += 1

print (str((100.0*badSeqs)/seqCount) + "percent or "
  + str(badSeqs) + " out of " + str(seqCount) + " were not translated.")
```

Conveniently these sequences cover the same region as our protein alignment and thus no trimming is required. We now want to search them against themselves and all other global genes. First we need to create a sequence file to perform the search.
```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/ape_data/
cat ./mosaic_data/Protein_recheniwoi_from_larremore_renamed.fasta ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta > ./mosaic_processed_data/ape_data/combined_ape_454.fasta
```

```{python, eval=FALSE}
from mungo.fasta import FastaReader

with open("./mosaic_processed_data/ape_data/combined_ape_454_renamedForMosaic.fasta", 'w') as outfile:
  with open("./mosaic_processed_data/ape_data/combined_ape_454_mapping.txt", 'w') as mapping:
    count=1
    for h,s in FastaReader("./mosaic_processed_data/ape_data/combined_ape_454.fasta"):
      if "SYpttt15" in h:
        outfile.write(">target_seq"+str(count)+"\n" +s+"\n")
      else:
        outfile.write(">db_seq"+str(count)+"\n" +s+"\n")
      mapping.write("seq"+str(count)+"\t"+h.split()[0]+"\n")
      count+=1
```

Now run mosaic
```{bash, eval=FALSE}
cd ./mosaic_processed_data/ape_data/
seq="combined_ape_454_renamedForMosaic.fasta"
fasta="combined_ape_454_renamedForMosaic"
nohup /home/users/allstaff/tonkin-hill.g/global_var_manuscript/mosaic/mosaic -ma -seq $seq -aa -tag $fasta -group 2 db target -target target -del 0.0166765773591 -eps 0.273305573191 -rec 0.014 > ${fasta}_output.log &
cd ../..
```

To run the proportion script we need to add the Reichenowi isolates into the binary matrix.
```{r, eval=FALSE}
binary_df <- fread("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary.txt"
      , data.table = FALSE
      , header = TRUE)
lines <- read_lines("./processed_data/recheniwoi_from_larremore_renamed.fasta")
lines <- lines[grepl("^>", lines)]
lines <- str_replace(lines, ">", "")

t <- cbind(lines, matrix(0, length(lines), ncol(binary_df)-1))
colnames(t) <- colnames(binary_df)
binary_df <- rbind(binary_df, t, stringsAsFactors=FALSE)
binary_df$SYpttt15 <- c(rep(0, nrow(binary_df)-length(lines)), rep(1, length(lines)))
write.table(binary_df, file="./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary_withApe.txt"
            , quote = FALSE, row.names = FALSE
            , col.names = TRUE, sep="\t")

```

```{bash, eval=FALSE}
python ./scripts/calculate_expected_proportions.py --otu ./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary_withApe.txt --map ./mosaic_processed_data/ape_data/combined_ape_454_mapping.txt --out ./mosaic_processed_data/ape_data/mixture_ape.txt --pos ./mosaic_processed_data/ape_data/combined_ape_454_renamedForMosaic_post.txt --verbose --isolate SYpttt15
```

```{r}
isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))

#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]
isolateInformation <- isolateInformation[,c("Isolate", "Country")]
isolateInformation <- rbind(isolateInformation, data.frame(Isolate="SYpttt15", Country = "Ape"))

#Proportion file
proportion <- fread("./mosaic_processed_data/ape_data/mixture_ape.txt", data.table = FALSE)
colnames(proportion) <- c("Target", "Reference", "Proportion")

prop_merge <- merge(proportion, isolateInformation, by.x="Reference", by.y = "Isolate", all.x=TRUE)

summ_location <- prop_merge %>% group_by(Country) %>%
  summarise(
    num.isolates = n(),
    sum.prop = sum(Proportion),
    avg.proportion = mean(Proportion)
  )

summ_location$reWeighted <- summ_location$avg.proportion/sum(summ_location$avg.proportion)

summ_location$Country <- factor(summ_location$Country
                                       , levels = c("Ape", "Uganda", "Ghana", "Gabon", "Iran", "Thailand", "PNG", "Peru", "FrenchGuiana", "Venezuela", "Colombia"))
gg <- ggplot(summ_location, aes(x=Country, y=reWeighted, fill=Country))
gg <- gg + geom_bar(stat = "identity")
gg <- gg + theme_bw() + theme(axis.text.x = element_text(angle = 90))
gg
```

```{r}
summary_prop_table <- avg_proportions %>% group_by_(.dots=c("Origin","Country")) %>%
  summarise(
    mean_=mean(reWeighted),
    sd=sd(reWeighted)
  )
d <- acast(summary_prop_table, Origin~Country, value.var="mean_")
ggcorrplot(t(d), lab = TRUE)
```

Now to ignore the alignment to itself
```{r}
summ_location <- prop_merge %>% group_by(Country) %>%
  summarise(
    num.isolates = n(),
    sum.prop = sum(Proportion),
    avg.proportion = mean(Proportion)
  )

summ_location <- summ_location[summ_location$Country!="Ape",]
summ_location$reWeighted <- summ_location$avg.proportion/sum(summ_location$avg.proportion)

summ_location$Country <- factor(summ_location$Country
                                       , levels = c("Uganda", "Ghana", "Gabon", "Iran", "Thailand", "PNG", "Peru", "FrenchGuiana", "Venezuela", "Colombia"))
gg <- ggplot(summ_location, aes(x=Country, y=reWeighted, fill=Country))
gg <- gg + geom_bar(stat = "identity")
gg <- gg + theme_bw() + theme(axis.text.x = element_text(angle = 90))
gg
```

####Other Ape parasites

We now look at the relationship between the other ape parasites of Larremore et al.
First combine P.Gaboni, P.Billcolinsi and the mix of P.praefalciparum and P.Olderi with 3D7, HB3 and DD2.
```{bash, eval=FALSE}
mkdir ape_processed_data
```

First we collect all the sequences before translating them into protein sequence and remove those that can not be successfully translated.
```{python, eval=FALSE}
from mungo.sequence import sixFrameTranslation
from mungo.fasta import FastaReader
import os, sys
from collections import defaultdict

with open("./ape_processed_data/combined_ape_3d7_DD2_HB3.fasta", 'w') as outfile:
  for f in ["SYpte37_C2.fasta",  "SYptt15_C1.fasta",  "SYptt20_C3.fasta",  "SYptt75_C2.fasta",  "SYptt79_C3.fasta"]:
    iso = f.split(".")[0]
    f = "./data/" + f
    for h,s in FastaReader(f):
      read_name = iso + "." + h.split()[0]
      outfile.write(">" + read_name + "\n" + s + "\n")
  for h,s in FastaReader("./data/combined_all_454_noPOR.fasta"):
    iso = h.split(".")[0]
    if iso in ["3D7","HB3","DD2"]:
      outfile.write(">"+h+"\n"+s+"\n")

num_stops = 0
seqCount = 0
badSeqs = 0
bad_lengths = []

inputfile = "./ape_processed_data/combined_ape_3d7_DD2_HB3.fasta"
output_file = "./ape_processed_data/Protein_combined_ape_3d7_DD2_HB3.fasta"

with open(output_file + "_BadSeqs", 'w') as badfile:
  with open(output_file, 'w') as outfile:
    for h,s in FastaReader(inputfile):
      stops = 9999
      translation = sixFrameTranslation(s)
      for frame in translation:
        st = translation[frame].count('*')
        if st < stops:
          best = frame
          stops = st
      if stops <= num_stops:
        outfile.write(">" + h + " frame_" + str(best) + "\n")
        outfile.write(translation[best] + "\n")
      else:
        badSeqs += 1
        bad_lengths.append(len(s))
        badfile.write(">" + h + "\n")
        badfile.write(s + "\n")
      seqCount += 1

print (str((100.0*badSeqs)/seqCount) + "percent or "
  + str(badSeqs) + " out of " + str(seqCount) + " were not translated.")

successfully_translated = {}

for h,s in FastaReader(output_file):
  successfully_translated[h.split()[0]]=s

#Create filtered DNA sequence file for clustering
with open("./ape_processed_data/DNA_combined_ape_3d7_DD2_HB3.fasta", 'w') as outfile:
  for h,s in FastaReader(inputfile):
    if h in successfully_translated:
      outfile.write(">"+h+"\n"+s+"\n") 
```
98.74% of the sequences were successful translated.

```{bash, eval=FALSE}
python ./scripts/clusterDBLa.py -o ./ape_processed_data/ -r ./ape_processed_data/DNA_combined_ape_3d7_DD2_HB3.fasta --cpu 30 --verbose
```

Get protein translations of centroids and rename ready for mosaic
```{python, eval=FALSE}
from mungo.fasta import FastaReader

centroids=set()
for h,s in FastaReader("./ape_processed_data/DNA_combined_ape_3d7_DD2_HB3_renamed_centroids.fasta"):
  centroids.add(h.split(";sample=")[0])

count=0
with open("./ape_processed_data/Protein_combined_ape_3d7_DD2_HB3_renamed_centroids_mapping.txt", 'w') as mapping:
  with open("./ape_processed_data/Protein_combined_ape_3d7_DD2_HB3_renamed_centroids.fasta", 'w') as outfile:
    for h,s in FastaReader("./ape_processed_data/Protein_combined_ape_3d7_DD2_HB3.fasta"):
      if h.split()[0] in centroids:
        count+=1
        read="seq"+str(count)
        mapping.write(read + "\t" + h.split()[0] + "\n")
        outfile.write(">target_"+read+"\n"+s+"\n")
```

Now run mosaic
```{bash, eval=FALSE}
cd ./ape_processed_data/
seq="Protein_combined_ape_3d7_DD2_HB3_renamed_centroids.fasta"
fasta="Protein_combined_ape_3d7_DD2_HB3_renamed_centroids"
/home/users/allstaff/tonkin-hill.g/global_var_manuscript/mosaic/mosaic -ma -seq $seq -aa -tag $fasta -group 2 db target -target target -del 0.0166765773591 -eps 0.273305573191 -rec 0.014 | tee ${fasta}_output.log
cd ..
```

Now we can look at the resulting proportions
```{bash, eval=FALSE}
mkdir ./ape_processed_data/proportions
mkdir ./ape_processed_data/proportions/temp_files

python ./scripts/calculate_expected_proportions.py --otu ./ape_processed_data/DNA_combined_ape_3d7_DD2_HB3_renamed_otuTable_binary.txt --map ./ape_processed_data/Protein_combined_ape_3d7_DD2_HB3_renamed_centroids_mapping.txt --out_dir ./ape_processed_data/proportions/ --pos ./ape_processed_data/*post.txt --verbose --compute_all --temp_dir ./ape_processed_data/proportions/temp_files/ --seq ./ape_processed_data/Protein_combined_ape_3d7_DD2_HB3.fasta
```

```{r}
isolateInformation <- data.frame(
  Isolate=c("SYpte37_C2",  "SYptt75_C2",  "SYptt15_C1",  "SYptt20_C3",  "SYptt79_C3", "HB3", "DD2", "3D7"),
  Species=c("P.gaboni","P.gaboni","P.reichenowi","P.billcollinsi","P.billcollinsi","P.falciparum","P.falciparum","P.falciparum"),
  stringsAsFactors = FALSE)
  
#Proportion file
prop_files <- Sys.glob("./ape_processed_data/proportions/*proportions.txt")
avg_proportions <- data.frame()
for (f in prop_files){

  proportion <- fread(f, data.table = FALSE)
  colnames(proportion) <- c("Target", "Reference", "Proportion")
prop_merge <- merge(proportion, isolateInformation, by.x="Reference", by.y = "Isolate", all.x=TRUE)

  sum_location <- prop_merge %>% group_by(Species) %>%
    summarise(
      num.isolates = n(),
      sum.prop = sum(Proportion),
      avg.proportion = mean(Proportion)
    )

  sum_location$reWeighted <- sum_location$avg.proportion/sum(
    sum_location$avg.proportion)
  iso <- gsub(".*proportions/","",f)
  iso <- gsub("_proportions.txt","",iso)
  sum_location$Isolate <- rep(iso, nrow(sum_location))
  avg_proportions <- rbind(avg_proportions, sum_location)
}

colnames(isolateInformation) <- c("Isolate","Origin")
avg_proportions <- merge(avg_proportions, isolateInformation, by.x="Isolate", by.y="Isolate", all.x=TRUE)

gg <- ggplot(avg_proportions, aes(x=Isolate, y=reWeighted, fill=Species))
gg <- gg + facet_grid(Species~Origin, scales = "free_x", space = "free_x")
gg <- gg + coord_cartesian(ylim=c(0,1))
gg <- gg + geom_bar(stat = "identity")
gg <- gg + theme_bw() + theme(axis.text.x = element_text(angle = 90))
gg <- gg + ylab("Proportion match found by JHMM")
gg

```

##A closer investigation of South America

We now look at how the South American isolates as a mixture of non-south American isolate sequences. This helps to look at genes that are conserved in a subset of SA countries which obscures the distances to the African ones.

First lets load the binary matrix.
```{r, eval=FALSE}
isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))

#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]
isolateInformation <- isolateInformation[,c("Isolate", "Country")]

otuTable <- fread("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary.txt"
      , data.table = FALSE
      , header = TRUE)
otuMatrix <- as.matrix(otuTable[,2:ncol(otuTable)])
rownames(otuMatrix) <- otuTable$`#OTU ID`
SA_isolates <- isolateInformation$Isolate[isolateInformation$Country %in% c("Peru", "FrenchGuiana", "Venezuela", "Colombia")]

SA_types <- rownames(otuMatrix)[rowSums(otuMatrix[,colnames(otuMatrix) %in% SA_isolates])>0]
non_SA_types <- rownames(otuMatrix)[rowSums(otuMatrix[,!(colnames(otuMatrix) %in% SA_isolates)])>0]
unique_to_SA_types <- SA_types[!(SA_types %in% non_SA_types)]
length(unique_to_SA_types)
write.table(unique_to_SA_types, file="./mosaic_data/unique_SA_types.txt"
            , sep=",", col.names = FALSE, row.names = FALSE, quote = FALSE)
```

Now to set up the files needed to run mosaic
```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/SA_proportions
```

```{python, eval=FALSE}
from mungo.fasta import FastaReader

seqs = []
SA_unique_seqs = []
SA_types = set()

with open("./mosaic_data/unique_SA_types.txt", 'rU') as infile:
  for line in infile:
    SA_types.add(line.strip())

count=0
for h,s in FastaReader("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta"):
  if h in SA_types:
    SA_unique_seqs.append(("seq"+str(count), h, s))
  else:
    seqs.append(("seq"+str(count), h, s))
  count+=1

for target in SA_unique_seqs:
  with open("./mosaic_processed_data/SA_proportions/Combined_454_tessema_centroids_reducedRegion_SA_target_"+target[0]+".fasta",'w') as outfile:
    outfile.write(">target_"+target[0]+"\n"+target[2]+"\n")
    for seq in seqs:
      outfile.write(">db_"+seq[0]+"\n"+seq[2]+"\n")

with open("./mosaic_processed_data/SA_proportions/Combined_454_tessema_centroids_reducedRegion_SA_target_mapping.txt",'w') as outfile:
  for target in SA_unique_seqs:
    outfile.write(target[0] + "\t" + target[1] + "\n")
  for seq in seqs:
    outfile.write(seq[0] + "\t" + seq[1] + "\n")
```

```{r, eval=FALSE}
noSA_otuMatrix <- otuMatrix[,!(colnames(otuMatrix) %in% SA_isolates)]

for (iso in SA_isolates){
  print(iso)
  t <- data.frame(`otu`=rownames(noSA_otuMatrix), target=otuMatrix[,iso], noSA_otuMatrix)
  outfile <- paste(c("./mosaic_processed_data/SA_proportions/Combined_454_tessema_centroids_reducedRegion_SA_target_", iso, ".txt"), collapse = "")
  cat(paste(c("#OTU ID",iso,colnames(noSA_otuMatrix)), collapse="\t")
      , file=outfile,sep="\n")
  write.table(t, file=outfile
            , quote = FALSE, row.names = FALSE
            , col.names = FALSE, sep="\t", append = TRUE)
}
```

```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/SA_proportions/temp_files

for iso in ./mosaic_processed_data/SA_proportions/*target*.txt; 
do 
iso=${iso##*_};
iso=${iso%.*};
echo $iso
python ./scripts/calculate_expected_proportions.py --otu "./mosaic_processed_data/SA_proportions/Combined_454_tessema_centroids_reducedRegion_SA_target_${iso}.txt" --map ./mosaic_processed_data/SA_proportions/Combined_454_tessema_centroids_reducedRegion_SA_target_mapping.txt --out_dir ./mosaic_processed_data/SA_proportions/ --pos ./mosaic_processed_data/SA_proportions/mosaic_results/*post.txt --verbose --isolate $iso --temp_dir ./mosaic_processed_data/SA_proportions/mosaic_results/temp_files/ --seq ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta --no_match_self
done
```

```{r}

isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))

#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]
isolateInformation <- isolateInformation[,c("Isolate", "Country")]

#Proportion file
prop_files <- Sys.glob("./mosaic_processed_data/SA_proportions/*proportions.txt")
avg_proportions <- data.frame()
for (f in prop_files){

  proportion <- fread(f, data.table = FALSE)
  colnames(proportion) <- c("Target", "Reference", "Proportion")
prop_merge <- merge(proportion, isolateInformation, by.x="Reference", by.y = "Isolate", all.x=TRUE)

  sum_location <- prop_merge %>% group_by(Country) %>%
    summarise(
      num.isolates = n(),
      sum.prop = sum(Proportion),
      avg.proportion = mean(Proportion)
    )

  sum_location$reWeighted <- sum_location$avg.proportion/sum(
    sum_location$avg.proportion)
  sum_location$Country <- factor(sum_location$Country
                                       , levels = c("Ape", "Uganda", "Ghana",
                                                    "Gabon", "Iran", "Thailand",
                                                    "PNG", "Peru", "FrenchGuiana",
                                                    "Venezuela", "Colombia"))
  iso <- gsub(".*proportions/","",f)
  iso <- gsub("_proportions.txt","",iso)
  sum_location$Isolate <- rep(iso, nrow(sum_location))
  avg_proportions <- rbind(avg_proportions, sum_location)
}

colnames(isolateInformation) <- c("Isolate","Origin")
avg_proportions <- merge(avg_proportions, isolateInformation, by.x="Isolate", by.y="Isolate", all.x=TRUE)

gg <- ggplot(avg_proportions, aes(x=Isolate, y=reWeighted, fill=Country))
gg <- gg + facet_grid(Country~Origin, scales = "free_x", space = "free_x")
gg <- gg + coord_cartesian(ylim=c(0,1))
gg <- gg + geom_bar(stat = "identity")
gg <- gg + theme_bw() + theme(axis.text.x = element_text(angle = 90))
gg

```

```{r}
summary_prop_table <- avg_proportions %>% group_by_(.dots=c("Origin","Country")) %>%
  summarise(
    mean=mean(reWeighted),
    sd=sd(reWeighted)
  )

summary_prop_table$Origin <- factor(summary_prop_table$Origin
                                    , levels=c("Peru", "FrenchGuiana","Venezuela", "Colombia"))
summary_prop_table$Country <- factor(summary_prop_table$Country
                                    , levels=c("Uganda", "Ghana","Gabon", "Iran", "Thailand","PNG"))

gg <- ggplot(summary_prop_table, aes(Country, Origin, fill = mean)) 
gg <- gg + geom_raster()
gg <- gg + scale_fill_gradientn(colours=c("#FFFFFFFF","#FF0000FF"))
gg <- gg + geom_text(data=summary_prop_table,aes(x=Country, y=Origin
                    ,label=paste(sprintf("%0.3f", round(mean, digits = 3))
                                 , sprintf("%0.4f", round(sd, digits = 4))
                                 , sep = "±"))
                    , size=3)
gg <- gg + theme(axis.text.x = element_text(angle = 45, hjust = 1))
gg
```

##Global without Peru or Venezuela
```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/SA_Ven_Peru_proportions
```

First lets load the binary matrix and remove the Peruvian and Venezuelan isolates.
```{r, eval=FALSE}
isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))

#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]
isolateInformation <- isolateInformation[,c("Isolate", "Country")]

otuTable <- fread("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary.txt"
      , data.table = FALSE
      , header = TRUE)
otuMatrix <- as.matrix(otuTable[,2:ncol(otuTable)])
rownames(otuMatrix) <- otuTable$`#OTU ID`
PeruVen_isolates <- isolateInformation$Isolate[isolateInformation$Country %in% c("Peru", "Venezuela")]

noPeruVenn_otuMatrix <- otuMatrix[,!(colnames(otuMatrix) %in% PeruVen_isolates)]
noPeruVenn_otuMatrix <- noPeruVenn_otuMatrix[rowSums(noPeruVenn_otuMatrix)>0,]

outfile <- "./mosaic_processed_data/SA_Ven_Peru_proportions/Combined_454_tessema_centroids_reducedRegion_SA_noPeruVenezuela_binaryMatrix.txt"
cat(paste(c("#OTU ID",colnames(noPeruVenn_otuMatrix)), collapse="\t")
      , file=outfile,sep="\n")
write.table(noPeruVenn_otuMatrix, file=outfile
            , quote = FALSE, row.names = TRUE
            , col.names = FALSE, sep="\t", append = TRUE)
```

Now we want to search the types that are singletons that are found in Colombia and French Guiana.
```{r, eval=FALSE}
otuTable <- fread(outfile
      , data.table = FALSE
      , header = TRUE)
otuMatrix <- as.matrix(otuTable[,2:ncol(otuTable)])
rownames(otuMatrix) <- otuTable$`#OTU ID`
ColFrench_isolates <- isolateInformation$Isolate[isolateInformation$Country %in% c("Colombia", "FrenchGuiana")]

ColFrench_types <- rownames(otuMatrix)[rowSums(otuMatrix[,colnames(otuMatrix) %in% ColFrench_isolates])>0]
non_ColFrench_types <- rownames(otuMatrix)[rowSums(otuMatrix[,!(colnames(otuMatrix) %in% ColFrench_isolates)])>0]
unique_to_ColFrench_types <- ColFrench_types[!(ColFrench_types %in% non_ColFrench_types)]
length(unique_to_ColFrench_types)
write.table(unique_to_ColFrench_types, file="./mosaic_processed_data/SA_Ven_Peru_proportions/unique_Colombia_FrenchGuiana_types.txt"
            , sep=",", col.names = FALSE, row.names = FALSE, quote = FALSE)

write.table(rownames(otuMatrix), file="./mosaic_processed_data/SA_Ven_Peru_proportions/keep_types_noPeruVenezuela.txt"
            , sep=",", col.names = FALSE, row.names = FALSE, quote = FALSE)

write.table(ColFrench_isolates, file="./mosaic_processed_data/SA_Ven_Peru_proportions/ColFrench_isolates.txt"
            , sep=",", col.names = FALSE, row.names = FALSE, quote = FALSE)

```

Now to set up the files needed to run mosaic

```{python, eval=FALSE}
from mungo.fasta import FastaReader

seqs = []
SA_unique_seqs = []
SA_types = set()
keep_type = set()

#Load type we want to search
with open("./mosaic_processed_data/SA_Ven_Peru_proportions/unique_Colombia_FrenchGuiana_types.txt", 'rU') as infile:
  for line in infile:
    SA_types.add(line.strip())

#Load remaining types after the removal of Peru and Venezuela
with open("./mosaic_processed_data/SA_Ven_Peru_proportions/keep_types_noPeruVenezuela.txt", 'rU') as infile:
  for line in infile:
    keep_type.add(line.strip())
    

count=0
for h,s in FastaReader("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta"):
  if h not in keep_type: continue
  if h in SA_types:
    SA_unique_seqs.append(("seq"+str(count), h, s))
  seqs.append(("seq"+str(count), h, s))
  count+=1

for target in SA_unique_seqs:
  with open("./mosaic_processed_data/SA_Ven_Peru_proportions/Combined_454_tessema_centroids_reducedRegion_SA_PeruVen_target_"+target[0]+".fasta",'w') as outfile:
    outfile.write(">target_"+target[0]+"\n"+target[2]+"\n")
    for seq in seqs:
      if seq[0] == target[0]: continue
      outfile.write(">db_"+seq[0]+"\n"+seq[2]+"\n")

with open("./mosaic_processed_data/SA_Ven_Peru_proportions/Combined_454_tessema_centroids_reducedRegion_SA_PeruVen_target_mapping.txt",'w') as outfile:
  for seq in seqs:
    outfile.write(seq[0] + "\t" + seq[1] + "\n")
```


```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/SA_Ven_Peru_proportions/temp_files

ISOLATES="$(< ./mosaic_processed_data/SA_Ven_Peru_proportions/ColFrench_isolates.txt)"

for iso in $ISOLATES; do
echo $iso
python ./scripts/calculate_expected_proportions.py --otu ./mosaic_processed_data/SA_Ven_Peru_proportions/Combined_454_tessema_centroids_reducedRegion_SA_noPeruVenezuela_binaryMatrix.txt --map ./mosaic_processed_data/SA_Ven_Peru_proportions/Combined_454_tessema_centroids_reducedRegion_SA_PeruVen_target_mapping.txt --out_dir ./mosaic_processed_data/SA_Ven_Peru_proportions/ --pos ./mosaic_processed_data/SA_Ven_Peru_proportions/*post.txt --verbose --isolate $iso --temp_dir ./mosaic_processed_data/SA_Ven_Peru_proportions/temp_files/ --seq ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta --no_match_self
done
```

Now we can plot the results
```{r}
isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))

#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]
isolateInformation <- isolateInformation[,c("Isolate", "Country")]

#Proportion file
prop_files <- Sys.glob("./mosaic_processed_data/SA_Ven_Peru_proportions/*proportions.txt")
avg_proportions <- data.frame()
for (f in prop_files){

  proportion <- fread(f, data.table = FALSE)
  colnames(proportion) <- c("Target", "Reference", "Proportion")
prop_merge <- merge(proportion, isolateInformation, by.x="Reference", by.y = "Isolate", all.x=TRUE)

  sum_location <- prop_merge %>% group_by(Country) %>%
    summarise(
      num.isolates = n(),
      sum.prop = sum(Proportion),
      avg.proportion = mean(Proportion)
    )

  sum_location$reWeighted <- sum_location$avg.proportion/sum(
    sum_location$avg.proportion)
  sum_location$Country <- factor(sum_location$Country
                                       , levels = c("Ape", "Uganda", "Ghana",
                                                    "Gabon", "Iran", "Thailand",
                                                    "PNG", "Peru", "FrenchGuiana",
                                                    "Venezuela", "Colombia"))
  iso <- gsub(".*proportions/","",f)
  iso <- gsub("_proportions.txt","",iso)
  sum_location$Isolate <- rep(iso, nrow(sum_location))
  avg_proportions <- rbind(avg_proportions, sum_location)
}

colnames(isolateInformation) <- c("Isolate","Origin")
avg_proportions <- merge(avg_proportions, isolateInformation, by.x="Isolate", by.y="Isolate", all.x=TRUE)

gg <- ggplot(avg_proportions, aes(x=Isolate, y=reWeighted, fill=Country))
gg <- gg + facet_grid(Country~Origin, scales = "free_x", space = "free_x")
gg <- gg + coord_cartesian(ylim=c(0,1))
gg <- gg + geom_bar(stat = "identity")
gg <- gg + theme_bw() + theme(axis.text.x = element_text(angle = 90))
gg

```

```{r}
summary_prop_table <- avg_proportions %>% group_by_(.dots=c("Origin","Country")) %>%
  summarise(
    mean=mean(reWeighted),
    sd=sd(reWeighted)
  )

summary_prop_table$Origin <- factor(summary_prop_table$Origin
                                    , levels=c("FrenchGuiana", "Colombia"))
summary_prop_table$Country <- factor(summary_prop_table$Country
                                    , levels=c("Uganda", "Ghana","Gabon", "Iran", "Thailand","PNG","FrenchGuiana","Colombia"))

gg <- ggplot(summary_prop_table, aes(Country, Origin, fill = mean)) 
gg <- gg + geom_raster()
gg <- gg + scale_fill_gradientn(colours=c("#FFFFFFFF","#FF0000FF"))
gg <- gg + geom_text(data=summary_prop_table,aes(x=Country, y=Origin
                    ,label=paste(sprintf("%0.3f", round(mean, digits = 3))
                                 , sprintf("%0.4f", round(sd, digits = 4))
                                 , sep = "±"))
                    , size=3)
gg <- gg + theme(axis.text.x = element_text(angle = 45, hjust = 1))
gg
```

###Investigate the recombination hotspots
```{python, eval=FALSE}
import glob
from mungo.fasta import FastaReader
from collections import defaultdict
import numpy as np

mapping_dict = {}
with open("./mosaic_processed_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta_mapping.txt", 'rU') as infile:
  for line in infile:
    line=line.strip().split()
    mapping_dict[line[0]]=line[1]

target_jumps = defaultdict(list)
for align_file in glob.glob("./mosaic_processed_data/results_full/*.fasta_align.txt"):
  with open(align_file, 'rU') as infile:
    for line in infile:
      if "target_seq" in line:
        if "Target:" in line: continue
        pos = 0
        gaps = 0
        line = line.strip().split()
        read = mapping_dict[line[0].split("_")[1]]
        seq = line[1]
      if "db_seq" in line:
        align_len = len(line.strip().split()[1])
        pos += align_len
        if pos>=len(seq): continue #we've got to the end
        gaps += seq[(pos-align_len):pos].count("-")
        target_jumps[read].append(pos-gaps)

for h,seq in FastaReader("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta.fa"):
  if "consensus seq" in h:
    align_length = len(seq)
    break_point_counts = [0] * align_length
    continue
  h=h.split()[0] #remove frame annotation
  pos_count = 0
  aa_count = 0
  for aa in seq:
    if aa!="-":
      aa_count+=1
    pos_count+=1
    if (aa_count in target_jumps[h]) and (aa!="-"):
      break_point_counts[pos_count] = break_point_counts[pos_count] + 1

with open("./mosaic_data/breakpoint_jump_counts.txt", 'w') as outfile:
  outfile.write(",".join([str(c) for c in break_point_counts]))
```

```{r}
recombination <- unlist(fread("./mosaic_data/breakpoint_jump_counts.txt", data.table = FALSE)[1,])
lines <- read_lines("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta.fa")
lines <- lines[!grepl("^>.*", lines)]
lines <- unlist(lines[lines!=""])
occupancy <- str_split_fixed(lines, "", n=str_length(lines[[1]]))
occupancy <- occupancy!="-"
occupancy <- colSums(occupancy)/nrow(occupancy)

plot_data <- data.frame(position=1:length(recombination)
                        , recombination=recombination
                        , occupancy=occupancy
                        , stringsAsFactors = FALSE)
plot_data <- melt(plot_data, id.vars = c("position"))
gg <- ggplot(plot_data, aes(x=position, y=value), fill=variable)
gg <- gg + geom_bar(stat='identity')
gg <- gg + facet_wrap(~variable, scales = "free_y", ncol=1)
gg <- gg + theme_bw()
gg
```

####Session Information
This is only valid session information for the R code. Much of the remaining code was run on a high performance computing cluster at the Walter and Eliza Hall Institute.
```{r}
sessionInfo()
```