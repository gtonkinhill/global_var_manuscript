---
title: "Supplementary Methods (Mosaic)"
author: "Gerry Tonkin-Hill"
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 12
    fig_height: 8
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8,
                      echo=FALSE, warning=FALSE, message=FALSE,
                      tidy=TRUE)
```

##Mosaic model
The mosaic model is best run on protein sequence as the alignments will be more interpretable. It will also be more efficient as the algorithm complexity is of order $N^2 l^2$. Furthermore, we only want to compare isolates with isolates from other regions. Thus we remove lab isolates and those sequences that can not be translated. We say a sequence can be translated if the resulting protein sequence has no stop codons.

First we translate all sequences to identify those that can not be succesfully translated.
```{python, eval=FALSE}
from mungo.sequence import sixFrameTranslation
from mungo.fasta import FastaReader
import os, sys
from collections import defaultdict

num_stops = 0
seqCount = 0
badSeqs = 0
bad_lengths = []

inputfile = "./mosaic_data/combined_454_tessema.fas"
output_file = "./mosaic_data/Protein_combined_454_tessema_renamed.fasta"

with open(output_file + "_BadSeqs", 'w') as badfile:
  with open(output_file, 'w') as outfile:
    for h,s in FastaReader(inputfile):
      stops = 9999
      translation = sixFrameTranslation(s)
      for frame in translation:
        st = translation[frame].count('*')
        if st < stops:
          best = frame
          stops = st
      if stops <= num_stops:
        outfile.write(">" + h + " frame_" + str(best) + "\n")
        outfile.write(translation[best] + "\n")
      else:
        badSeqs += 1
        bad_lengths.append(len(s))
        badfile.write(">" + h + "\n")
        badfile.write(s + "\n")
      seqCount += 1

print (str((100.0*badSeqs)/seqCount) + "percent or "
  + str(badSeqs) + " out of " + str(seqCount) + " were not translated.")
```

97.14% of the sequences were successfully translated. 
We would now like to filter out those that did not translate along with the labratory isolates before clustering the remaining sequences.

```{python, eval=FALSE}
from mungo.fasta import FastaReader

successfully_translated = {}
keep=set()

for h,s in FastaReader("./mosaic_data/Protein_combined_454_tessema_renamed.fasta"):
  successfully_translated[h]=s

#Remove lab isolates
with open("./mosaic_data/Protein_NoLab_combined_454_tessema.fasta", 'w') as outfile:
  for h in successfully_translated:
    if "DD2" in h.split(".")[0]: continue
    if "HB3" in h.split(".")[0]: continue
    if "3D7" in h.split(".")[0]: continue
    outfile.write(">"+h+"\n"+successfully_translated[h]+"\n")
    keep.add(h.split()[0])

#Create filtered DNA sequence file for clustering
with open("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/combined_454_tessema.fas"):
    if h in keep:
      outfile.write(">"+h+"\n"+s+"\n") 
```

We only want to look at a single copy of DBLa type as this will drastically reduce the computational complexity of the problem. We cluster the filtered DNA sequences at 96% id using the same pipline as was used in the binary type analysis.

```{bash, eval=FALSE}
python ./scripts/clusterDBLa.py -o ./mosaic_data/ -r ./mosaic_data/DNA_NoLab_translateable_combined_454_tessema.fasta --cpu 30 --verbose
```

We now extract the protein sequences that correspond to the centroids of the clusters.

```{python, eval=FALSE}
from mungo.fasta import FastaReader

centroids=set()
for h,s in FastaReader("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_centroids.fasta"):
  centroids.add(h.split(";sample=")[0])

with open("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/Protein_NoLab_combined_454_tessema.fasta"):
    if h.split()[0] in centroids:
      outfile.write(">"+h+"\n"+s+"\n")
```

The sequences from Tessema et al cover a larger region of the DBLa domain. In inferring the mixtures of each isolate we will later assume that each sequence is covering approximately the same region. Cosequently, we need to trim the sequences of Tessema et al to match those in the 454 data. This was not important in the DNA clustering as the pipeline we used allowed for terminal gaps at no cost. To achieve this we align all the protein sequences using gismo which was found to handle VAR protein sequences better than many other aligners.

```{bash, eval=FALSE}
./scripts/gismo_unlimited \ ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta \ -maxseq=40000
```

The trailing sequence from the PNG reads will be trimmed in this alignment and we will be left with a consensus region from. 

```{python, eval=FALSE}
from mungo.fasta import FastaReader

count = 0
with open("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta", 'w') as outfile:
  for h,s in FastaReader("./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta.fa"):
    count+=1
    if count<=1: continue
    h=h.split()[0]
    s=s.replace("-","")
    s=s.upper()
    outfile.write(">"+h+"\n"+s+"\n")
```

Now we have representative protein sequences for each DBLa type we can start to fit the jumpping hidden Markov model. Unfortunately the complete Baum-Welch algorithm  requires a number of full all-vs-all searches using the forward-backward algorithms and this is very computaitonaly instesive. Due to computational contraints this was impractable. Consequently, the slightly less accurate but much fast viterbi training algorithm was used.

We first split the dataset into subset to be searched in parralel on our computing cluster.
```{bash, eval=FALSE}
mkdir mosaic_processed_data
cp ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta ./mosaic_processed_data/
cd mosaic_processed_data
python ../scripts/split_into_runs.py Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta
cd ..
```

Now we perform the Viterbi training algorithm
Iterate until convergence:
0) Choose an initial set of parameters
1) Compute the Viterbi paths of all sequences
2) Count frequencies of events and calculate parameters
3) Update -> 1) 
4) Stop when the major paramters del and eps change by less than 1%. 

The algorithm converged after 7 iterations.

This gives us the non-jump parameters which we fix when estimating a likelihood surface for the jump parameter. Due to computational constraints we could not estimate the jump parameter by computing and all-vs-all search at each parameter point. Thus we resort to searching 1000 randomly chosen target sequences against the entire dataset. That is 1000-vs-all. First, 1000 sequences are randomly selected as targets.

```{bash, eval=FALSE}
cd mosaic_processed_data
python ../scripts/random_target_sample.py Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta
cd ..
```

```{bash, eval=FALSE}
import glob
from collections import defaultdict

jump_llk = defaultdict(list)
for logfile in glob.glob("*jump*_output.log"):
  jump = float(logfile.split("jump")[1].split("_output")[0])
  with open(logfile, 'rU') as infile:
    for line in infile:
      if "Maximum log-likelihood =" in line:
          jump_llk[jump].append(float(line.split("=")[1].strip()))

with open("jump_llk.csv", 'w') as outfile:
  for j in sorted(jump_llk.keys()):
    outfile.write(",".join([str(j)]+[str(l) for l in jump_llk[j]]) + "\n")
```

Lets investigate the jump log-likelihoods
```{r}
library(data.table)
library(ggplot2)
library(boot)

llk <- read.table("./mosaic_data/jump_llk.csv"
                  , sep=",", header = FALSE, fill = TRUE)

llk_matrix <- data.matrix(llk[,2:ncol(llk)])

rownames(llk_matrix) <- llk[,1]

sum_stat <- function(x,i){
  t<-x[i]
  sum(t, na.rm = TRUE)
}

llk_df <- data.frame(jump=rownames(llk_matrix),
           llk=rowSums(llk_matrix, na.rm = TRUE),
           CI.L=unlist(apply(llk_matrix, 1
              , function(x){boot.ci(boot(x, sum_stat, 1000),type="perc")$percent[[4]]} )),
           CI.R=unlist(apply(llk_matrix, 1
              , function(x){boot.ci(boot(x, sum_stat, 1000),type="perc")$percent[[5]]} )),
           stringsAsFactors = FALSE)

ggplot(llk_df, aes(x=jump, y=llk, group=1)) +
    geom_line() +
    geom_errorbar(width=.1, aes(ymin=CI.L, ymax=CI.R)) +
    geom_point(shape=21, size=3, fill="white") + 
    ylim(c(-110000,-90000))

rownames(llk_matrix)[which.max(rowSums(llk_matrix, na.rm = TRUE))]
```

We can now run the full model in an all vs all fashion
```{bash, eval=FALSE}

```

Now calculate the proportions
```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/proportions
python ./scripts/calculate_expected_proportions.py --otu ./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary.txt --map ./mosaic_processed_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta_mapping.txt --out ./mosaic_processed_data/proportions/prop_Col1.txt --pos ./mosaic_processed_data/results_full/*post.txt --verbose --isolate Col1
```


##Ape parasite
It is also interesting to look out how the Ape reichenowi isolate from Larremore et al looks.
First we need to reformat the reichenowi sequences.
```{python, eval=FALSE}
from mungo.fasta import FastaReader
from mungo.sequence import sixFrameTranslation
import os, sys
from collections import defaultdict

with open("./processed_data/recheniwoi_from_larremore_renamed.fasta", 'w') as outfile:
  for h,s in FastaReader("./data/all_recheniwoi_from_larremore.fasta"):
    h=h.replace(",","")
    h="_".join(h.split())
    outfile.write(">SYpttt15." + h + "\n" + s + "\n")

num_stops = 0
seqCount = 0
badSeqs = 0
bad_lengths = []

inputfile = "./processed_data/recheniwoi_from_larremore_renamed.fasta"
output_file = "./mosaic_data/Protein_recheniwoi_from_larremore_renamed.fasta"

with open(output_file + "_BadSeqs", 'w') as badfile:
  with open(output_file, 'w') as outfile:
    for h,s in FastaReader(inputfile):
      stops = 9999
      translation = sixFrameTranslation(s)
      for frame in translation:
        st = translation[frame].count('*')
        if st < stops:
          best = frame
          stops = st
      if stops <= num_stops:
        outfile.write(">" + h + " frame_" + str(best) + "\n")
        outfile.write(translation[best] + "\n")
      else:
        badSeqs += 1
        bad_lengths.append(len(s))
        badfile.write(">" + h + "\n")
        badfile.write(s + "\n")
      seqCount += 1

print (str((100.0*badSeqs)/seqCount) + "percent or "
  + str(badSeqs) + " out of " + str(seqCount) + " were not translated.")
```

Conveniently these sequences cover the same region as our protein alignment and thus no trimming is required. We now want to search them against themselves and all other global genes. First we need to create a sequence file to perfrom the search.
```{bash, eval=FALSE}
mkdir ./mosaic_processed_data/ape_data/
cat ./mosaic_data/Protein_recheniwoi_from_larremore_renamed.fasta ./mosaic_data/Protein_NoLab_translateable_combined_454_tessema_centroids_reducedRegion.fasta > ./mosaic_processed_data/ape_data/combined_ape_454.fasta
```

```{python, eval=FALSE}
from mungo.fasta import FastaReader

with open("./mosaic_processed_data/ape_data/combined_ape_454_renamedForMosaic.fasta", 'w') as outfile:
  with open("./mosaic_processed_data/ape_data/combined_ape_454_mapping.txt", 'w') as mapping:
    count=1
    for h,s in FastaReader("./mosaic_processed_data/ape_data/combined_ape_454.fasta"):
      if "SYpttt15" in h:
        outfile.write(">target_seq"+str(count)+"\n" +s+"\n")
      else:
        outfile.write(">db_seq"+str(count)+"\n" +s+"\n")
      mapping.write("seq"+str(count)+"\t"+h.split()[0]+"\n")
      count+=1
```

Now run mosaic
```{bash, eval=FALSE}
cd ./mosaic_processed_data/ape_data/
seq="combined_ape_454_renamedForMosaic.fasta"
fasta="combined_ape_454_renamedForMosaic"
nohup /home/users/allstaff/tonkin-hill.g/global_var_manuscript/mosaic/mosaic -ma -seq $seq -aa -tag $fasta -group 2 db target -target target -del 0.0166765773591 -eps 0.273305573191 -rec 0.014 > ${fasta}_output.log &
cd ../..
```

To run the proportion script we need to add the Reichenowi isolates into the binary matrix.
```{r}
library(data.table)
library(stringr)
library(readr)
binary_df <- fread("./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary.txt"
      , data.table = FALSE
      , header = TRUE)
lines <- read_lines("./processed_data/recheniwoi_from_larremore_renamed.fasta")
lines <- lines[grepl("^>", lines)]
lines <- str_replace(lines, ">", "")

t <- cbind(lines, matrix(0, length(lines), ncol(binary_df)-1))
colnames(t) <- colnames(binary_df)
binary_df <- rbind(binary_df, t, stringsAsFactors=FALSE)
binary_df$SYpttt15 <- c(rep(0, nrow(binary_df)-length(lines)), rep(1, length(lines)))
write.table(binary_df, file="./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary_withApe.txt"
            , quote = FALSE, row.names = FALSE
            , col.names = TRUE, sep="\t")

```

```{bash, eval=FALSE}
python ./scripts/calculate_expected_proportions.py --otu ./mosaic_data/DNA_NoLab_translateable_combined_454_tessema_renamed_otuTable_binary_withApe.txt --map ./mosaic_processed_data/ape_data/combined_ape_454_mapping.txt --out ./mosaic_processed_data/ape_data/mixture_ape.txt --pos ./mosaic_processed_data/ape_data/combined_ape_454_renamedForMosaic_post.txt --verbose --isolate SYpttt15
```

```{r}
library(dplyr)
library(ggplot2)

isolateInformation <- fread("./data/isolate_information.csv"
                            , header=TRUE
                            , data.table = FALSE)
#Add in country information
isolateInformation$Country <- unlist(lapply(isolateInformation$Location
                                            , function(x) {
                                              str_split(x,  "_", n=2)[[1]][[1]]}))

#Remove duplicate entries of isolates that have been sequenced more than once
isolateInformation <- isolateInformation[!duplicated(isolateInformation$Isolate),]
isolateInformation <- isolateInformation[,c("Isolate", "Country")]
isolateInformation <- rbind(isolateInformation, data.frame(Isolate="SYpttt15", Country = "Ape"))

#Proportion file
proportion <- fread("./mosaic_processed_data/ape_data/mixture_ape.txt", data.table = FALSE)
colnames(proportion) <- c("Target", "Reference", "Proportion")

prop_merge <- merge(proportion, isolateInformation, by.x="Reference", by.y = "Isolate", all.x=TRUE)

summ_location <- prop_merge %>% group_by(Country) %>%
  summarise(
    num.isolates = n(),
    sum.prop = sum(Proportion),
    avg.proportion = mean(Proportion)
  )

summ_location$reWeighted <- summ_location$avg.proportion/sum(summ_location$avg.proportion)

summ_location$Country <- factor(summ_location$Country
                                       , levels = c("Ape", "Uganda", "Ghana", "Gabon", "Iran", "Thailand", "PNG", "Peru", "FrenchGuiana", "Venezuela", "Colombia"))
gg <- ggplot(summ_location, aes(x=Country, y=reWeighted, fill=Country))
gg <- gg + geom_bar(stat = "identity")
gg <- gg + theme_bw() + theme(axis.text.x = element_text(angle = 90))
gg
```

Now to ignore the alignment to itself
```{r}
summ_location <- prop_merge %>% group_by(Country) %>%
  summarise(
    num.isolates = n(),
    sum.prop = sum(Proportion),
    avg.proportion = mean(Proportion)
  )

summ_location <- summ_location[summ_location$Country!="Ape",]
summ_location$reWeighted <- summ_location$avg.proportion/sum(summ_location$avg.proportion)

summ_location$Country <- factor(summ_location$Country
                                       , levels = c("Uganda", "Ghana", "Gabon", "Iran", "Thailand", "PNG", "Peru", "FrenchGuiana", "Venezuela", "Colombia"))
gg <- ggplot(summ_location, aes(x=Country, y=reWeighted, fill=Country))
gg <- gg + geom_bar(stat = "identity")
gg <- gg + theme_bw() + theme(axis.text.x = element_text(angle = 90))
gg
```